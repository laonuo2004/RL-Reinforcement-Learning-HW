# 简介

## 任务介绍

峡谷漫步的目标是：训练一个智能体，让其在对地图不断地探索中学习移动策略，减少碰撞障碍物，以最少的步数**从起点走到终点**，可能会有附属任务——**收集宝箱**。

## 场景介绍

峡谷漫步使用64*64的网格化的地图（智能体每步移动的距离是一网格），地图中包含**起点、终点、道路、障碍物**和**宝箱**等元素。

| 元素 | 说明 |
| --- | --- |
| 英雄 | 环境中存在英雄单位，智能体可以控制英雄在地图中进行移动。本环境采用鲁班作为英雄角色。 |
| 起点 | 任务开始时，英雄单位在起点出现。 |
| 终点 | 任务设置的目的地，当英雄抵达终点时，任务结束。 |
| 道路 | 英雄可以在道路中通过前、后、左、右四个方向移动进行试探，每次移动的距离是一个网格。 |
| 障碍物 | 英雄遇到障碍物时无法继续前进，需要调整方向开启新的探索。 |
| 宝箱 | 如果用户给任务配置了宝箱，则智能体可以通过控制英雄拾取宝箱增加积分，每个宝箱获得100积分，地图中共有10个可配置宝箱的点位。 |
| 视野范围 | 智能体的视野范围是以自己为中心，5*5范围内的所有格子。 |

![峡谷漫步场景](./images/峡谷漫步场景.png)

## 规则介绍

总积分 = 终点积分 + 步数积分 + 宝箱积分

- 终点积分：到达终点即获得150积分。
- 步数积分：(最大步数 - 完成步数) * 奖励系数0.2。任务设置有最大步数，当智能体到达终点时，根据最大步数和任务完成步数计算步数积分。
- 宝箱积分：每获得一个宝箱，即可增加100积分。

注意：若在最大步数内没有走到终点（包括最大步数），则判定为任务超时。超时任务的总积分为0。

# 环境介绍

> 💡 腾讯开悟提供了标准的强化学习开发流程，在正式进入开发之前，请先了解并熟悉[强化学习开发流程](#强化学习开发流程)。

## 强化学习环境

### 环境配置

在智能体和环境的交互中，首先会调用`env.reset`方法，该方法接受一个`usr_conf`参数，这个参数通过读取`train_env_conf.toml`文件的内容来实现定制化的环境配置。因此，用户可以通过修改`train_env_conf.toml`文件中的内容来调整环境配置。

```python
# usr_conf为用户传入的环境配置
observation, state = env.reset(usr_conf=usr_conf)
```

`train_env_conf.toml`中包含以下信息：

| 数据名 | 数据类型 | 数据描述 | 默认值 |
| --- | --- | --- | --- |
| **start** | [int,int] | 起点位置坐标 | [29,9] |
| **end** | [int,int] | 终点位置坐标 | [11,55] |
| **treasure_random** | bool | 是否生成随机宝箱，false表示固定宝箱，true表示随机宝箱。开启固定，则使用treasure_id生成固定宝箱。 若开启随机，则使用treasure_count生成随机宝箱。 | false |
| **treasure_count** | int | 生成随机宝箱时的宝箱数量，仅在treasure_random = true时生效，取值范围为0~10。 | 0 |
| **treasure_id** | list of int | 生成固定宝箱时的宝箱编号，仅在treasure_random = false时生效，取值范围为0~9。 | [ ] |
| **max_step** | int | 单局最大步数，取值范围为1~2000。无特殊需求不建议设置，过大的值会导致训练缓慢。 | 1000 |

```toml
[env_conf]
# 起点位置坐标，数组，只包含两个元素，其中第一个元素表示x坐标，第二个元素表示y坐标，
# x和y的取值范围分别为0~64，默认值为[29,9]，
# 起点位置坐标和终点位置坐标不能重复。
start = [29, 9]

# 终点位置坐标，数组，只包含两个元素，其中第一个元素表示x坐标，第二个元素表示y坐标，
# x和y的取值范围分别为0~64，默认值为[11,55]，
# 起点位置坐标和终点位置坐标不能重复。
end = [11, 55]

# 是否生成随机宝箱，布尔值，false表示固定宝箱，true表示随机宝箱。
# 若开启固定，则使用treasure_id生成固定宝箱。
# 若开启随机，则使用treasure_count生成随机宝箱。
# 默认值为false。
treasure_random = false

# 生成随机宝箱时的宝箱数量，仅在treasure_random = true时生效，整型，取值范围为0~10，默认值为0。
treasure_count = 0

# 生成固定宝箱时的宝箱编号，仅在treasure_random = false时生效，数组，取值范围为0~9，默认值为[]。
# 注意需要排除起点和终点编号，如果需要固定生成0个宝箱则传入[]。
treasure_id = []

# 单局最大步数，整型，取值范围为1~2000，默认值为1000。
max_step = 1000
```

> **补充说明**：
> 1. **环境配置仅在训练时生效**，请按上表所述进行`train_env_conf.toml`文件的配置。若配置错误，`env.reset`会在调用超时后返回None，无法获取任务状态，训练不会正常进行，您可以通过监控界面查看**错误日志**进行排查。
> 2. `usr_conf`使用的是`train_env_conf.toml`文件中的默认配置。
> 3. 模型评估时，用户需要通过开悟平台创建评估任务并完成任务的环境配置。
> 4. 我们提供的示例代码默认dynamic_programming、monte_carlo采用无宝箱配置。
> 5. 以下为每个**宝箱id**和**坐标**（原点为地图左下角）：

| 描述 | 宝箱0 | 宝箱1 | 宝箱2 | 宝箱3 | 宝箱4 | 宝箱5 | 宝箱6 | 宝箱7 | 宝箱8 | 宝箱9 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **坐标** | [19, 14] | [9, 28] | [9, 44] | [42, 45] | [32, 23] | [49, 56] | [35, 58] | [23, 55] | [41, 33] | [54, 41] |

![峡谷之森地图](./images/峡谷之森地图.png)

### 环境信息

在调用`env.reset`与`env.step`接口时，会返回环境当前的状态，如下所示。

```python
# usr_conf为用户传入的环境配置
observation, state = env.reset(usr_conf=usr_conf)
# step返回的`frame_no`是任务帧数，`terminated`表示任务结束，`truncated`表示任务中断(超时或异常)
frame_no, observation, score, terminated, truncated, state = env.step(hero_actions)
```

下面会对这些数据进行描述

#### 环境返回信息

| 数据名 | 数据类型 | 数据描述 |
| --- | --- | --- |
| frame_no | int | 当前帧数 |
| observation | Observation | 环境局部观测 |
| score | ScoreInfo | 得分信息 |
| terminated | int | 表示任务结束，即达到任务最大步数 |
| truncated | int | 表示任务中断(超时或异常) |
| state | State | 环境全局状态 |

#### 环境局部观测

`observation`包含环境返回的特征数据`feature`、当前合法动作`legal_act`。
以下是从环境中获取到的原始observation描述：

| 数据名 | 数据类型 | 数据描述 |
| --- | --- | --- |
| feature | list of float | 特征数据 |
| legal_act | list of int | 合法动作 |

其中`feature`的具体描述请参考[强化学习智能体](#强化学习智能体)部分

#### 环境全局状态

`state`包含环境返回的全局信息。

| 数据名 | 数据类型 | 数据描述 |
| --- | --- | --- |
| frame_state | FrameState | 环境帧数据 |
| game_info | GameInfo | 环境信息 |

### 动作空间

峡谷漫步的动作空间比较简单，只有上下左右四个动作，智能体以网格为单位进行四个方向的移动，每次移动一格。

`env.step(act)` 里的 `act` 的合法取值为 {0, 1, 2, 3}，对应的动作如下：

```yaml
action_space:
  0: UP
  1: DOWN
  2: LEFT
  3: RIGHT
```

### 得分信息

`env.step(act)` 返回的 `score` 是在当前状态下执行动作 `act` 智能体所获得的分数，分数的计算详见[规则介绍](#规则介绍)。

> **注意**：积分是用于衡量模型在环境中的表现，也作为衡量强化学习训练后的模型的优劣，与强化学习里的奖励要区别开。

### 环境监控信息

监控面板中包含了**env**模块，表示**环境指标**数据，详细说明如下。

| 指标名称 | 说明 |
| --- | --- |
| **score** | 该面板包含两个指标：<br> total_score：任务结束时的总积分，若任务超时，则该局积分为0。<br>treasure_score：任务结束时收集到的宝箱奖励。 |
| **steps** | 该面板包含两个指标：<br> max_steps：任务设置的最大步数。<br> finished_steps：任务结束时所用的步数。若任务超时，则该局完成步数等于最大步数。 |
| **treasure** | 该面板包含两个指标：<br> total_treasures：任务设置的宝箱个数。<br> collected_treasures：任务结束时收集到的宝箱个数。 |
| **treasure_random** | 宝箱是否随机。若为0则表示宝箱位置固定，若为1则表示宝箱位置随机。 |

---

## 强化学习智能体

### 特征处理

对应`observation["feature"]`

#### 特征子类说明

| 数据名 | 数据类型 | 数据维度 | 数据描述 |
| --- | --- | --- | --- |
| state | int | [0] | 智能体当前位置状态，state = x * 64 + z |
| pos_row | list of int | [1:65] | 智能体当前位置横坐标的 one-hot 编码 |
| pos_col | list of int | [65:129] | 智能体当前位置纵坐标的 one-hot 编码 |
| end_dist | int | [129:130] | 智能体当前位置相对于终点的离散化距离，0-6, 数字越大表示越远 |
| treasure_dist | list of int | [130:140] | 智能体当前位置相对于宝箱的离散化距离，0-6, 数字越大表示越远 |
| obstacle_flat | list of int | [140:165] | 智能体局部视野中障碍物的信息 (一维化)，1 表示障碍物, 0 表示可通行 |
| treasure_flat | list of int | [165:190] | 智能体局部视野中宝箱的信息 (一维化)，1 表示宝箱, 0 表示没有宝箱 |
| end_flat | list of int | [190:215] | 智能体局部视野中终点的信息 (一维化)，1 表示终点, 0 表示非终点 |
| memory_flat | list of int | [215:240] | 智能体局部视野中的记忆信息 (一维化)，取值范围[0,1], 一个格子每走过一次, 记忆值+0.1 |
| treasure_status | list of int | [240:250] | 宝箱的状态，1 表示可以被收集, 0 表示不可被收集(未生成或者已经收集过), 长度为 10 |

上述数据是将环境的**原始数据**经过简单的**特征处理**之后得到的**特征数据**，包含**位置信息**，**距离信息**, **宝箱收集信息**和**视野域信息**。

#### 位置信息

[地图介绍](#地图介绍)里提到, 峡谷漫步地图本质上是一个 **64x64** 的网格，智能体的位置可以用一个二维的坐标来表示，但是为了简化，我们采用下面的方式将二维坐标等价转换为一维的状态值：

$$state = x * 64 + z$$

另外，我们提供了另外一种方式来表征位置信息，即坐标的 **one-hot encoding**：

```python
pos_row = [0] * 64
pos_row[self.pos[0]] = 1
pos_col = [0] * 64
pos_col[self.pos[1]] = 1
```

#### 距离信息

`end_dist` 和 `treasure_dist` 是用来表征智能体当前位置相对于终点和宝箱的距离。该距离是两点之间的最短路径距离，即智能体需要走多少步才能到达，在此距离的基础上做了离散化处理，用 **[0, 6]** 共 **7** 个整数来表示距离的长短，数字越大，距离越长。

> **注意**: 由于环境最多生成 **10** 个宝箱，所以 `treasure_dist` 是 **10** 维，其 `index` 和宝箱的 id 一致；如果某个宝箱未生成，则对应的距离值为 **999**。

#### 宝箱状态信息

峡谷漫步的宝箱总共有 **10** 个可能出现的位置，每个位置对应了一个宝箱id，详情见[环境配置](#环境配置)。宝箱状态信息 (treasure_status) 是一个长度为 **10** 的列表，**1** 表示可以被收集，**0** 表示不可被收集(未生成或者已经收集过)。宝箱的 id 对应了该宝箱在列表里的 index，详情见下例：

- 初始状态时，`treasure_status` 为 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
- 当环境启动时，会根据用户传入的 `usr_conf` 生成宝箱，生成的宝箱状态会变为 1。比如前 5 个宝箱被生成了，那 `treasure_status` 为 [1, 1, 1, 1, 1, 0,  0,  0,  0,  0]
- 随着环境的进行，宝箱2 和 宝箱4 被收集，则 `treasure_status` 变为 [1, 1, 0, 1, 0, 0, 0, 0, 0, 0]

#### 视野域信息

上面的所有信息都是**向量特征**，而视野域信息是**图特征**，包含障碍物图，终点图，和记忆图。

**视野域**是指以智能体所在位置为中心，分别向上下左右四个方向拓宽 VIEW (default = 2) 格数的一个正方形的**局部观察域** (defualt = 5 x 5)。

视野域中会标注出障碍物、宝箱、终点的位置：有则标注为 1，无则标注为 0，我们分别以障碍物和宝箱为例子

```python
obstacle_map = [[1, 1, 1, 1, 1],
                [0, 0, 1, 1, 1],
                [0, 0, 0, 1, 1],
                [0, 0, 0, 0, 1],
                [1, 1, 1, 1, 1]]
```

obstacle_map矩阵的中心位置为智能体所在位置，1 代表有障碍物，0 代表无障碍物可以通行。

```python
treasure_map = [[0, 0, 0, 1, 0],
                [0, 0, 0, 0, 0],
                [0, 0, 0, 0, 0],
                [0, 0, 0, 0, 0],
                [1, 0, 0, 0, 0]]                
```

treasure_map矩阵的中心位置为智能体所在位置，1 代表有宝箱，0 代表无宝箱。

**记忆图**同样是一个 5 x 5 的局部观察域，值限制在 [0, 1]，初始化为 0。智能体每走到一个位置，该位置对应的值 +0.1，最大为 1，以此来表征智能体对历史探索过的信息的记忆。

> **注意**: obs 里的 `obstacle_flat`, `treasure_flat`, `end_flat`, `memory_flat` 是把上述图特征经过一维化后得到的长度为 25 的向量

---

### 算法介绍

我们在峡谷漫步代码包中提供了4个算法 **dynamic_programming**，**monte_carlo**，**sarsa**，**q_learning**，同时，我们还提供了一个**diy**模板算法文件夹，用户可在该文件夹中自定义算法实现。

#### Dynamic Programming 算法

Dynamic Programming基于贝尔曼方程，通过递归分解问题，将迷宫路径规划视为多阶段决策过程。通过值迭代或策略迭代算法，计算每个状态的最优值函数（V值）或最优策略。在已知环境完整模型和状态空间有限下，可以保证得到全局最优解。该算法缺点是计算复杂度较高，无法对未知环境进行建模。

#### Monte Carlo 算法

Monte Carlo 算法通过随机采样完整轨迹（从起点到终点），用经验平均回报估计状态值函数。无需环境模型，直接通过采样数据进行学习。原理简单，实现简单，无需环境模型，可适用于有较大随机性的环境。缺点是需大量采样，方差高，策略更新缓慢。

#### SARSA 算法

SARSA（State-Action-Reward-State-Action）是一种On-Policy​的时序差分（Temporal Difference, TD）强化学习算法。其核心思想是通过智能体与环境交互生成的实际动作序列来更新Q值函数。与Q-Learning不同，SARSA严格遵循当前策略（如ε-贪婪策略）选择动作，即在更新Q值时依赖实际执行的下一动作，而非假设选择最优动作。

#### ​Q-Learning 算法

Q-Learning 是一种Off-Policy​的强化学习算法，其目标是通过学习状态-动作值函数（Q 函数）来找到最优策略。Q 函数定义为在状态 s 下执行动作 a 后，智能体能获得的最大累积奖励期望值。通过最大化下一状态Q值更新当前Q表。利用贝尔曼最优方程，学习独立于当前策略的最优动作。它的缺点是：Q表存储限制（状态爆炸），无法处理连续空间。

### 算法监控信息

监控面板中包含了**algorithm**模块，展示**算法指标**数据，以下是本环境中提供的**dynamic_programming**、**monte_carlo**、**sarsa**、**q_learning**算法指标的详细说明。

| 指标名称 | 说明 |
| --- | --- |
| **reward** | 最近10局平均每局的累积回报，反应了智能体的能力，正常训练情况下指标应该是震荡向上。 |

---

### 模型保存限制策略

为了避免用户保存模型的频率过于频繁，开悟平台对模型保存会有安全限制，不同的任务会有不同的限制，限制规则详情如下：

- 保存模型的频率限制: 2次/分钟
- 单个任务保存模型的次数限制：（不同算法的限制不同）
  - Dynamic Programming：10次
  - Monte Carlo：100次
  - Q-learning：100次
  - Sarsa：100次。

---

### 模型评估模式

在峡谷漫步中，评估时用户需要在提交任务界面进行宝箱配置，具体配置如下：

```
# 默认值
宝箱数量：0
宝箱是否随机：否
最大步数：1000
```

另外，训练模式时，用户一般使用agent.predict方法进行决策；而在评估模式时，平台会调用agent.exploit方法进行决策，一般情况下，模型在训练和评估时的决策会因算法不同和用户设计不同，而有不同的行为，这部分由用户定义和实现。

---

# 强化学习开发流程

## 简介

在腾讯开悟平台中，一个训练任务的流程如下图所示：

![开发任务描述](./images/开发任务描述.png)

训练任务中包含环境和智能体两大要素，智能体包含一个可被训练的模型，智能体可以对环境给出的观测进行决策，这个决策作用于环境产生新的观测，此过程通过训练工作流控制，不断循环。

另一方面，训练工作流还要收集循环过程中产生的每一帧数据，将他们组合成样本数据，智能体可以将这些样本数据作为算法的输入，通过算法更新模型参数。

如果采用分布式训练，会启动多个容器，此时样本数据需要通过网络通信发送到训练容器（learner）中进行训练，所以要对样本数据进行编码以方便网络发送，同时智能体需要将learner容器上的模型通过网络同步回来。

腾讯开悟提供了标准的强化学习开发流程，帮助用户快速完成相关内容的开发。

**开发流程简介**

为保证训练任务正常进行，我们需要完成以下几部分的开发：

1. **[定义数据结构](#定义数据结构)**：一般情况下，环境产生的原始观测数据不能直接作为智能体的输入，并且不同的用户开发的智能体一般是不一样的，显然不同的智能体的决策、学习方法的输入输出也是不一样的，所以开发的第一步，我们应该定义智能体输入输出的数据结构。

   包括特征（ObsData）、动作（ActData）、样本（SampleData），其中`ObsData`和`ActData`分别作为智能体`predict`方法的输入和输出，`SampleData`作为智能体`learn`方法的输入。

2. **[实现特征处理和样本处理](#实现特征处理和样本处理)**：不同的用户实现不同的智能体可能会定义不同的数据结构，但是，环境接口输入输出的数据结构是固定的，因此环境接口的输入输出数据和智能体接口的输入输出数据需要进行转换，所以还需要用户实现这些数据结构的转换方法。

   包括：`observation_process`, `action_process`, `sample_process`。

3. **[算法开发](#算法开发)**：用户需要实现一个 agent，agent中需要实现模型（一般是神经网络模型）和算法（强化学习算法）。agent负责与环境交互，产生预测动作并训练模型。

4. **[实现强化学习训练工作流](#实现强化学习训练工作流)**：在实现了 `数据结构`，`数据处理函数`，`模型`和 `智能体` 以及其他方法（如奖励处理函数）后，我们还需要实现一个强化学习的**训练工作流**`workflow`，将所有组件组合起来完成强化学习训练，即智能体通过不断的与环境交互，获取样本数据，更新并迭代模型，直到模型收敛到我们想要的效果。另外，用户可以对训练参数进行配置，在分布式训练时，开悟平台会启动一个样本池，一个模型同步服务，这些组件的相关参数用户也可以根据自己的需求进行配置。

![开发流程](./images/开发流程.png)

**分布式训练**

在腾讯开悟平台，训练任务的运行分为单机和分布式，单机指的是训练涉及的所有组件(进程)都在一个计算节点上运行，分布式指的是训练涉及的所有组件(进程)会分布在不止一个计算节点上运行，节点之间通过网络组件进行通信，配合完成任务。

如果选择分布式训练模式，开悟平台会启动一个样本池（样本先进先出），用户的agent.learn(samples)调用将会把样本发送到样本池，训练容器会从样本池中采样样本samples将其传入agent.learn(samples)进行训练，此过程是自动的，用户无需开发额外代码。

由于sample的类型是用户定义的 `SampleData`，该类型无法直接进行网络传输，需要统一编码成 `Numpy.array`类型的数据。所以需要用户编写 `SampleData2NumpyData`函数实现 `SampleData`类型数据到 `Numpy.array`类型的转换，同时还要编写 `NumpyData2SampleData`函数实现 `Numpy.array`类型数据到 `SampleData`类型的转换，两个函数作为相对应的编码和解码函数，每一位数据都需要对齐，否则将产生数据错误，无法有效训练。

另外，由于模型在训练容器（learner）进行训练，用户需要按需在恰当时机从训练容器加载模型。开悟平台会在某个容器中启动一个模型同步服务，用户在`workflow`中调用agent.load_model(id="latest")将会加载最新模型，若希望加载中间模型则可以指定id，若希望加载随机模型则调用agent.load_model(id="random")。

最后，是用户可以配置的参数。根据SampleData转换成的Numpy.array的数据长度设置`<agent_算法名称>/conf/conf.py`中配置项`SAMPLE_DIM`的值。还有样本池和模型同步服务的参数。

分布式训练架构如下图：

![分布式训练架构](./images/分布式训练架构.png)

---

接下来，我们将详细介绍开发流程中每一个模块的实现逻辑和核心函数。

## 开发流程

首先，我们先简单了解下完整的代码包目录，后续的开发工作将在不同的文件中进行。

完整的代码包目录结构如下：

| 目录名 | 介绍 |
| --- | --- |
| **<agent_算法名称>** | 算法子目录，用户的大部分代码开发在这个目录中，一般一个代码包中会包含多个算法子目录 |
| **diy** | Do it yourself 用户自定义算法的子目录 |
| **conf** | 配置文件 |
| **train_test.py** | 代码正确性测试脚本 |

### 代码包目录介绍

各个算法的子目录结构如下：

| 目录/文件名 | 介绍 |
| --- | --- |
| **algorithm/** | 算法相关，主要是 算法 的实现，可以实现多个算法，详情见[算法开发](#算法开发) |
| **feature/** | 特征相关，主要包含用户自定义的数据结构和数据处理方法，以及特征和奖励的计算，详情见[实现特征处理和样本处理](#实现特征处理和样本处理) |
| **model/** | 模型相关，主要是模型的实现，是一个Model类 |
| **conf/** | 配置，用户可以增加配置或修改配置 |
| **agent.py** | 智能体相关，主要是 agent 的实现，包含预测等，详情见[智能体开发](#智能体开发) |
| **workflow/** | 强化学习的训练流程，详情见[强化学习流程](#强化学习训练流程开发) |

---

### 智能体开发

#### 定义数据结构

环境返回的原始观测信息 `obs`的数据结构可以参考协议，这里的 `obs` 已经做了一定的**数据预处理**工作，但是智能体是由用户设计和实现的，环境使用的`obs`, `act`等与智能体的输入输出是存在差异的，所以要先定义数据结构（类）再进行数据转换，包括：包括特征（ObsData）、动作（ActData）、样本（SampleData），这部分的代码，都需要实现在`<agent_算法名称>/feature/definition.py`中。

首先需要定义相关的数据结构（类）包含观测数据ObsData，动作数据ActData，和样本数据SampleData, 其中：

- ObsData和ActData分别表示智能体预测的输入和输出，将会由agent.predict使用；
- SampleData为样本的数据类型，样本数据将会被agent.learn中的代码进行处理用于模型的训练。

在`definition.py`文件中通过`create_cls`函数创建数据结构（类）。`create_cls`的第一个参数为类型名称，剩余参数为类的属性，属性默认值为None。这些数据结构（类）包含哪些属性完全由用户自定义，属性名称和属性数量没有限制。

**核心函数介绍**

##### `create_cls`

- **Introduction**: 用于动态创建一个类，这个类包含哪些属性完全由用户自定义，属性名称和属性数量没有限制
- **Parameters**:
  - `第一个参数`: 字符串类型，表示为定义类型的名称
  - `其余参数`: 定义类的属性名称，属性默认值为None

示例代码：

```python
# The create_cls function is used to dynamically create a class. The first parameter of the function is the type name,
# and the remaining parameters are the attributes of the class, which should have a default value of None.
# create_cls函数用于动态创建一个类，函数第一个参数为类型名称，剩余参数为类的属性，属性默认值应设为None
ObsData = create_cls("ObsData", 
    feature=None, 
)
ActData = create_cls("ActData", 
    action=None, 
)
SampleData = create_cls("SampleData", 
    state=None, 
)
```

> **注意**：必须使用`create_cls`这个函数创建这些类，若使用普通的类定义方法（class 类名）将无法在开悟平台正确运行。

---

#### 实现特征处理和样本处理

用户需要实现特征处理，动作处理，样本处理，和奖励设计函数，例如环境返回的数据属于原始观测数据，是无法直接作为智能体预测时的输入的，我们需要实现特征处理函数`observation_process`，将环境返回的原始观测数据转换成用户定义的ObsData。这部分的代码，都需要实现在`<agent_算法名称>/feature/definition.py`中。

需要实现特征处理和样本处理的函数有：`observation_process`, `action_process`, `sample_process`。

> **注意**：这三个函数都必须使用@attached装饰器，代码默认已实现，注意不要删除。@attached装饰器会将用户实现的函数注册到开悟框架，若未使用装饰器，训练无法正常进行。

**核心函数介绍**

##### `observation_process`

- **Introduction**: 将环境返回的原始观测数据转换成用户定义的ObsData类型数据
- **Parameters**:
  - `obs`: Observation类型，env.reset和env.step返回的原始观测数据
  - `state`: EnvInfo类型，env.reset和env.step返回的环境状态数据
- **Return type**:
  - ObsData类型，用户定义的ObsData类型的数据 

示例代码：

```python
@attached
def observation_process(obs, state=None):
    return ObsData(feature=feature, legal_act=legal_actions)
```

##### `action_process`

- **Introduction**: 将智能体预测返回的ActData类的数据转换成env.step能处理的动作数据
- **Parameters**:
  - `act_data`: ActData类型，用户定义的ActData类型的数据
- **Return type**:
  - env.step能处理的动作数据类型 

示例代码：

```python
@attached
def action_process(act_data):
    return act_data.act
```

##### `sample_process`

- **Introduction**: 将环境数据帧的集合转换为样本的集合
- **Parameters**:
  - `list_game_data`: list(Frame) 类型，使用用户自定义的Frame作为输入，因为样本一般进行批处理，所以传入列表
- **Return type**:
  - list(SampleData)类型，SampleData类型的数据组成的列表

示例代码：

```python
@attached
def sample_process(list_game_data):
    return [SampleData(**i.__dict__) for i in list_game_data]
```

为了支持分布式训练，样本数据需要进行网络传输，由于SampleData无法直接进行网络传输，需要先转换成Numpy的Array，待传输到对端之后再由np.Array转换成SampleData。所以用户需要实现两个转换函数`SampleData2NumpyData`和`NumpyData2SampleData`，这两个函数互为反函数。以下是代码包中这两个函数的示例代码：

> **注意**：这两个函数的实现都必须包含一个装饰器@attached

##### `SampleData2NumpyData`

- **Introduction**: 将SampleData转换为NumpyData
- **Parameters**:
  - `g_data`: SampleData 类型
- **Return type**:
  - Numpy.array类型

示例代码：

```python
@attached
def SampleData2NumpyData(g_data):
    return g_data.npdata
```

##### `NumpyData2SampleData`

- **Introduction**: 将NumpyData转换为SampleData
- **Parameters**:
  - `s_data`: Numpy.array 类型
- **Return type**:
  - SampleData类型

示例代码：

```python
@attached
def NumpyData2SampleData(s_data):
    return SampleData(npdata=s_data)
```

---

#### 奖励设计

这里的奖励特指强化学习中的Reward，注意要与环境反馈的Score进行区分。Score用于衡量玩家在任务中的表现，也作为衡量强化学习训练后的模型的优劣。

代码包里提供了一些奖励的实现，可以参考`<agent_算法名称>/feature/definition.py`里的`reward_shaping`函数修改各个奖励的权重。用户还可以在这个函数中去实现自己的reward设计，这部分非常开放，回报设计的依据不一定只是环境给出的信息，也可以是用户对问题的理解、经验或者知识，建议用户根据对问题和强化学习算法的理解，去设计和实现自己的reward。

**核心函数介绍**

##### `reward_shaping`

- **Introduction**: 该方法用于实现用户设计的奖励方案。
- **Parameters**:
  - 参数个数和类型不限制，可以是环境信息、智能体信息、用户的经验和知识等。
- **Return type**:
  - 数值类型，根据用户设计的奖励方案，计算出的最终reward。

---

#### 算法开发

在代码包中，我们提供了不同的算法子目录，给用户进行算法的实现，每个算法的开发流程都是一致的，接下来我们描述如何实现：

首先，如果我们需要实现一个神经网络模型，我们需要在文件`<agent_算法名称>/model/model.py`中实现一个`Model`类，即用pytorch实现一个神经网络模型。

然后，我们需要在文件`<agent_算法名称>/agent.py`中实现一个 `Agent`类。注意`Agent`类需要继承 `kaiwu_agent.agent.base_agent` 的 `BaseAgent` 类，`Agent`类的实现需要符合`BaseAgent`类的接口规范。或者，如果在`<agent_算法名称>/algorithm/`文件夹下实现了算法，`Agent`类需要继承此算法类。

> **注意**：`Agent`类必须使用@attached装饰器，代码默认已实现，注意不要删除。

示例代码：

```python
class BaseAgent:
    """
    Agent 的基类，所有的 Agent 都应该继承自这个类"""
    def __init__(self, agent_type="player", device=None, logger=None, monitor=None) -> None:
        raise NotImplementedError

    def learn(self, list_sample_data) -> dict:
        """
        用于学习的函数，接受一个 SampleData 的列表
        """
        raise NotImplementedError

    def predict(self, list_obs_data: list) -> list:
        """
        用于获取动作的函数，接受一个 ObsData 的列表, 返回一个动作列表
        """
        raise NotImplementedError

    def exploit(self, list_obs_data: list) -> list:
        """
        用于获取动作的函数，接受一个 ObsData 的列表, 返回一个动作列表
        """
        raise NotImplementedError

    def save_model(self, path, id='1'):
        raise NotImplementedError

    def load_model(self, path, id='1'):
        raise NotImplementedError
```

**核心函数介绍**

##### `predict`

- **Introduction**: 该方法通过调用模型进行预测，是智能体训练时调用的方法，一般是依策略的概率分布采样或引入随机概率。
- **Parameters**:
  - `list_obs_data`: list(ObsData) 类型，使用用户自定义的ObsData作为输入进行预测，predict可进行批预测，所以传入ObsData的列表
- **Return type**:
  - list(ActData) 类型，预测结果是用户定义的ActData类型，predict可进行批预测，所以类型是list

示例代码：

```python
"""
<agent_算法名称>/agent.py
"""
@attached
class Agent(BaseAgent):
    @predict_wrapper
    def predict(self, list_obs_data):
        return self.__predict_detail(list_obs_data, exploit_flag=False)
```

##### `exploit`

- **Introduction**: 该方法也通过调用模型进行预测，是智能体在评估时调用的方法，一般是选取策略中概率最高的动作或者策略认为最优的动作。
- **Parameters**:
  - `list_obs_data`: list(ObsData) 类型，使用用户自定义的ObsData作为输入进行预测，exploit可进行批预测，所以传入ObsData的列表
- **Return type**:
  - list(ActData) 类型，预测结果是用户定义的ActData类型，exploit可进行批预测，所以类型是list

示例代码：

```python
"""
<agent_算法名称>/agent.py
"""
@attached
class Agent(BaseAgent):
    @exploit_wrapper
    def exploit(self, list_obs_data):
        return self.__predict_detail(list_obs_data, exploit_flag=True)
```

##### `learn`

- **Introduction**: 该方法主要负责消费样本进行模型训练，需要实现核心的强化学习算法。
- **Parameters**:
  - `list_sample_data`: list(SampleData) 类型，使用用户自定义的SampleData作为输入进行训练，一般进行批训练，所以传入SampleData的列表

示例代码：

```python
"""
<agent_算法名称>/agent.py
"""
@attached
class Agent(BaseAgent):
    @learn_wrapper
    def learn(self, list_sample_data):
        # 算法详细代码不在此处展示
        loss = policy_loss + Config.VALUE_LOSS_COEFF * value_loss
        loss.backward()
        grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)
        # Gradient Descent
        # 梯度下降
        self.optimizer.step()
```

##### `save_model`

- **Introduction**: 该方法主要负责模型的保存，注意模型默认的前缀model.ckpt-不要改动，可以通过id区分不同的模型。如果我们的模型是标准的pytorch格式，`save_model`方法中可以直接调用torch.save。
- **Parameters**:
  - `path`: 字符串类型，表示保存模型的路径 
  - `id`: 字符串类型，表示保存模型的唯一id

示例代码：

```python
"""
<agent_算法名称>/agent.py
"""
@attached
class Agent(BaseAgent):
    @save_model_wrapper
    def save_model(self, path=None, id="1"):
        # To save the model, it can consist of multiple files,
        # and it is important to ensure that each filename includes the "model.ckpt-id" field.
        # 保存模型, 可以是多个文件, 需要确保每个文件名里包括了model.ckpt-id字段
        model_file_path = f"{path}/model.ckpt-{str(id)}.pkl"

        # Copy the model's state dictionary to the CPU
        # 将模型的状态字典拷贝到CPU
        model_state_dict_cpu = {k: v.clone().cpu() for k, v in self.model.state_dict().items()}
        torch.save(model_state_dict_cpu, model_file_path)

        self.logger.info(f"save model {model_file_path} successfully")
```

##### `load_model`

- **Introduction**: 该方法主要负责模型的保存，注意模型默认的前缀model.ckpt-不要改动，可以通过id区分不同的模型。如果我们的模型是标准的pytorch格式，`load_model`方法中可以直接调用torch.load。
- **Parameters**:
  - `path`: 字符串类型，表示载入模型的路径
  - `id`: 字符串类型，表示载入模型的唯一id

示例代码：

```python
"""
<agent_算法名称>/agent.py
"""
@attached
class Agent(BaseAgent):
    @load_model_wrapper
    def load_model(self, path=None, id="1"):
        # When loading the model, you can load multiple files,
        # and it is important to ensure that each filename matches the one used during the save_model process.
        # 加载模型, 可以加载多个文件, 注意每个文件名需要和save_model时保持一致
        model_file_path = f"{path}/model.ckpt-{str(id)}.pkl"
        self.model.load_state_dict(
            torch.load(model_file_path, map_location=self.device),
        )

        self.logger.info(f"load model {model_file_path} successfully")
```

> **注意**：
> 1. `Agent`类的五个核心方法必须使用相应的装饰器，例如learn方法必须使用@learn_wrapper装饰器。代码默认已实现，注意不要删除。
> 2. 如果在本地训练DIY算法时，遇到电脑卡顿，CPU占用过多的情况，请在算法文件夹下的`agent.py`中通过以下代码调整线程数，推荐设置为1。

```python
torch.set_num_threads(1)
torch.set_num_interop_threads(1)
```

---

### 强化学习训练工作流开发

完成了上述开发步骤之后，需要再实现一个强化学习的训练工作流 - `workflow`，来让智能体 `Agent` 和环境 `Environment` 不断的交互从而产生训练样本并更新模型。腾讯开悟平台的强化学习训练工作流包含：

1. 获取env和agent
2. 监控数据初始化
3. 进入训练主循环
   1. 使用用户自定义的配置调用env.reset获得环境的第一帧
   2. 进入环境的episode循环
      1. 调用observation_process进行特征处理，得到ObsData类型的数据
      2. 调用agent.predict, 执行智能体决策，得到ActData类型的数据
      3. 调用action_process将上一步的ActData类型数据转换为env能处理的动作
      4. 调用env.step，执行动作与环境交互, 获取下一帧的状态
      5. 计算 reward
      6. 收集当前帧的所有信息
      7. 若episode结束或达到训练条件，调用sample_process处理当前收集到的所有信息并生成样本
      8. 若有样本生成则调用agent.learn进行训练
      9. 调用agent.load_model从模型同步服务更新最新模型
   3. 以适当时间上报适当的监控数据
4. 训练结束，保存最终模型

![开发流程-2](./images/开发流程-2.png)

为了实现这个强化学习训练工作流，我们需要在文件`<agent_算法名称>/train_workflow.py`中实现一个 `workflow`方法。

**核心函数介绍**

##### `workflow`

- **Introduction**: 该函数实现强化学习训练工作流。
- **Parameters**:
  - `envs`: list类型，环境列表，开悟框架会通过调用开悟场景库，得到相应的环境, 并作为输入传入 `workflow`。
  - `agents`: list类型，智能体列表，通过调用用户实现的 `<agent_算法名称>/agent.py` 实例化 Agent, 并作为输入传入 `workflow`。
  - `logger`: Logger类型，开悟提供的日志组件，接口与常见的 `python` 的 `logging` 库一致。
  - `monitor`: Monitor类型，开悟提供的监控组件。

> **注意**：workflow函数需要装饰器@attached，该代码不能删除！

示例代码：

```python
"""
<agent_算法名称>/train_workflow.py
"""
@attached
def workflow(envs, agents, logger=None, monitor=None):
    env, agent = envs[0], agents[0]
    episode_num_every_epoch = 1

    for epoch in range(epoch_num):
        epoch_total_rew = 0
        data_length = 0
        for g_data in run_episodes(episode_num_every_epoch, env, agent, g_data_truncat, logger):
            data_length += len(g_data)
            total_rew = sum([i.rew for i in g_data])
            epoch_total_rew += total_rew
            agent.learn(g_data)
            g_data.clear()
```

下面是一个任务循环的实现，基本是开发者之前实现的各个组件的调用：

```python
# 任务循环
done = False
collector = list()
while not done:
    # Agent performs inference, gets the predicted action for the next frame
    # Agent 进行推理, 获取下一帧的预测动作
    act_data = agent.predict(list_obs_data=[obs_data])

    # Unpack ActData into action
    # ActData 解包成动作
    act = action_process(act_data)

    # Interact with the environment, execute actions, get the next state
    # 与环境交互, 执行动作, 获取下一步的状态
    frame_no, _obs, score, terminated, truncated, _state = env.step(act)
    if _obs == None:
        break

    step += 1

    # Feature processing
    # 特征处理
    _obs_data = observation_process(_obs, _state)

    # Disaster recovery
    # 容灾
    if truncated and frame_no == None:
        break

    # Calculate reward
    # 计算 reward
    reward = reward_shaping(
            frame_no,
            score,
            terminated,
            truncated,
            obs,
            _obs,
    )

    done = terminated or truncated

    # Construct frame
    # 构造游戏帧，为构造样本做准备
    frame = Frame(
        obs=obs_data.feature,
        _obs=_obs_data.feature,
        act=act,
        rew=reward,
        done=done,
    )

    collector.append(frame)

    # If the game is over, the sample is processed and sent to training
    # 如果游戏结束，则进行样本处理，将样本送去训练
    if done:
        if len(collector) > 0:
            collector = sample_process(collector)
            # 返回样本数据, agent会调用agent.learn(g_data)进行训练
            yield collector
        break

    # Status update
    # 状态更新
    obs_data = _obs_data
    obs = _obs
    state = _state
```

下面是监控功能的一个展示：

```python
@attached
def workflow(envs, agents, logger=None, monitor=None):
    # 此处省略一些代码
    last_report_monitor_time = 0
    monitor_data = dict()
    if monitor and monitor_data:
        now = time.time()
        if now - last_report_monitor_time >= 60:
            monitor.put_data({os.getpid(): monitor_data})
            last_report_monitor_time = now
```

在上面的例子中可以看到，我们通过**时间间隔上报监控**，时间间隔＞60时，上报一些算法指标。

monitor和logger由`workflow`传入，用户可以根据需要在自己的代码中使用。

---

#### 模型保存

用户可通过以下方式保存模型：

1. 直接使用代码包中提供的`workflow`示例代码（默认包含模型保存逻辑）。
2. 在`workflow`代码中的自行调用`agent.save_model`保存中间模型。

> 💡 虽然`agent.save_model`接受`path`和`id`两个参数，但在`workflow`中调用该接口传入的参数会被框架覆盖成实际的模型保存路径以及最新的训练步数。
> 
> 开悟平台对模型保存会有安全限制，限制规则详情请参考**开发指南-强化学习智能体-模型保存限制策略**。

---

## 工具

### 模型评估

当训练产出模型后，用户可以在腾讯开悟平台上创建模型评估任务，以评估模型的能力。不同环境的评估任务支持设置的环境参数有所不同，详情请查看**开发指南-强化学习智能体-模型评估模式**。

---

### 代码调试

在代码包的根目录，我们提供了代码测试脚本`train_test.py`，该脚本将使用算法文件夹下`train_workflow.py`中的`workflow`进行一次训练，当训练步数＞0时判定本次代码测试通过。通过启动一次训练，脚本能够迅速验证流程中的各个环节是否正确进行，确保训练逻辑的准确性。

为避免训练模型时出现因代码问题导致的错误，我们建议用户在正式训练前一定要对代码进行测试。操作如下：

1. 将`train_test.py`文件中`algorithm_name`的值修改为需要测试的算法名，算法名需要是`algorithm_name_list`中的一个。
2. 进入IDE工具栏的【运行与调试】工具，点击下图所示绿色箭头的 **运行** 按钮。启动后，IDE会开始对代码进行测试，并将运行结果输出到右侧面板下方的终端区域，以方便用户进行观察和分析。

![code_test](./images/code_test.png)

在代码测试过程中如果遇到错误，则测试流程自动中止。此时用户可以根据下方的终端面板查看错误信息，根据错误信息定位代码的问题。

如果没有遇到错误，则代码测试流程会在一次强化训练结束后自动终止（几分钟左右，请耐心等待），并在下方的终端面板提示Train test succeed。

# 监控介绍

在腾讯开悟平台的训练管理页面，我们提供了**查看监控**功能，点击后，即可在新标签页中打开监控面板，如下图所示。你可以通过查看监控数据实时定位自己的训练进程，从而帮助大家评更快更准确的找到问题所在。

![监控面板](./images/监控面板.png)

---

## 监控面板介绍

在监控面板中，包括 **错误日志数量** 和 **监控指标图** 两部分内容。

**错误日志数量**：在该模块中，可以看到训练过程中每个模块的错误日志数量。点击模块卡片可以进入日志详情页，查看该模块的错误日志信息。

**监控指标图**：在该模块中，可以看到四类数据指标，分别是**basic**（基础指标）、**algorithm**（算法指标）、**env**（环境指标）、**diy**（自定义指标）。

| 指标分类 | 说明 |
| --- | --- |
| **basic** | 包括强化学习训练过程中的标准数据和资源使用数据。 |
| **hardware** | 和硬件相关的数据指标，体现硬件资源的利用率。 |
| **algorithm** | 和算法相关的数据指标，不同算法上报的指标可能会有所不同。 |
| **env** | 和环境相关的数据指标，不同环境上报的指标不同。 |
| **diy** | 用户自行上报的数据指标。 |

### basic（基础指标）

| 指标名称 | 说明 |
| --- | --- |
| **train_global_step** | 训练的累计步数，即`agent.learn`的调用次数。取决于各算法的具体实现。 |
| **predict_succ_cnt** | 采样预测的累计帧数，即`agent.predict`的调用次数。 |
| **sample_production_and_consumption_ratio** | 等于训练步数除以采样预测的累计帧数。 |
| **episode_cnt** | 已经结束的任务个数。 |

### hardware（硬件指标）

| 指标名称 | 说明 |
| --- | --- |
| **cpu_usage** | cpu使用率，该面板包含两个指标：<br> aisvr的cpu使用占比<br>learner的cpu使用占比。 |
| **gpu_usage** | gpu使用率，该面板包含两个指标：<br> aisvr的gpu使用占比<br>learner的gpu使用占比。 |
| **gpu_memory** | gpu显存使用率。 |
| **ram_usage** | 容器内存大小使用率。如果超过了一定限制会引起进程OOM掉，引起训练任务失败。 |
| **disk_usage** | 容器的磁盘大小使用率，一般是SAS盘。如果超过一定限制会导致容器被驱逐，引起训练任务失败。 |

### algorithm（算法指标）

在不同环境中，提供的算法各不相同，且每种算法对应的指标也有所区别。有关算法指标的详细介绍，请参考[开发指南-强化学习智能体-算法监控信息](#算法监控信息)。

### env（环境指标）

不同环境的指标各不相同，有关环境指标的详细介绍，请参考[开发指南-强化学习环境-环境监控信息](#环境监控信息)。

### diy（自定义指标）

我们提供了五个自定义指标`diy_1`至`diy_5`以便用户可以上报自己想要监控的数据。

![diy初始化](./images/diy初始化.png)

如上图红圈中所示，打开`train_workflow.py`，即可发现我们在此进行了`diy_1`至`diy_5`指标的初始化上报，如需更新此指标，只需在下图中的代码中添加`diy_1`至`diy_5`的新数据，即可完成上报更新。