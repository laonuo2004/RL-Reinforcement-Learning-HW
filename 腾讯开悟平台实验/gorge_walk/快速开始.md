# 动态规划算法 - 快速开始指南

## 修改总结

### 核心改进

1. **状态空间扩展**：从 4,096 扩展到 4,194,304（考虑宝箱收集）
2. **奖励优化**：-0.2/步（与评分机制一致），+100/宝箱，+150终点
3. **完整实现**：包含宝箱收集的状态转移函数

### 修改的文件

- ✅ `agent_dynamic_programming/conf/conf.py` - 状态空间配置
- ✅ `agent_dynamic_programming/agent.py` - 状态编码
- ✅ `agent_dynamic_programming/algorithm/algorithm.py` - 核心算法

### 新增文件

- 📄 `test_state_encoding.py` - 状态编码测试
- 📄 `实验详细分析.md` - 完整技术文档
- 📄 `动态规划实现说明.md` - 实现说明
- 📄 `快速开始.md` - 本文件

## 使用步骤

### 步骤1：测试状态编码

```bash
cd gorge_walk
python test_state_encoding.py
```

**预期输出**：
```
============================================================
测试状态编码和解码
============================================================

测试用例1：起点 [29, 9]，所有宝箱可收集
  位置ID: 1865
  宝箱状态: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
  编码后状态: 1910783
  编码/解码正确性: True

...

状态空间统计
============================================================
  总状态数: 4194304
  策略表大小: 16777216
  内存估计 (float32): 67.11 MB
```

### 步骤2：配置测评参数

**第一次测评**（`agent_dynamic_programming/conf/train_env_conf.toml`）：
```toml
start = [29, 9]
end = [11, 55]
treasure_id = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
treasure_random = false
max_step = 1999
```

### 步骤3：本地测试

修改 `train_test.py`：
```python
algorithm_name = "dynamic_programming"
```

运行测试：
```bash
python train_test.py
```

**预期行为**：
- 开始值迭代计算
- 输出迭代进度日志
- 约数十分钟后收敛
- 保存模型

### 步骤4：查看日志

训练日志会显示：
```
Starting value iteration with state_size=4194304
Iteration 0/100, delta=150.000, iter_time=45s, elapsed=45s
Iteration 10/100, delta=23.457, iter_time=44s, elapsed=452s
...
Converged at iteration 45 with delta=0.001
Value iteration completed in 2035s (45 iterations)
```

### 步骤5：提交训练

在腾讯开悟平台：
1. 上传代码包
2. 选择 `dynamic_programming` 算法
3. 创建训练任务
4. 等待训练完成（最多1小时）
5. 查看模型指标

### 步骤6：评估测试

使用第一次测评参数进行评估：
- 起点：[29, 9]
- 终点：[11, 55]  
- 宝箱：10个全部生成
- 最大步数：1999

## 预期效果

### 理论最优策略

动态规划将找到：
- 从起点到终点的路径
- 途中收集所有10个宝箱
- 最小化总步数

### 预期分数

**保守估计**：
```
收集宝箱：10 × 100 = 1000分
到达终点：150分
步数积分：假设200步，(1999-200) × 0.2 = 360分
总分：1000 + 150 + 360 = 1510分
```

**对比当前最好成绩**：
- Q-Learning：1027分
- 预期提升：**+47%**

## 关键参数

### 算法参数（`conf/conf.py`）

```python
STATE_SIZE = 4,194,304    # 总状态数
GAMMA = 0.9               # 折扣因子
THETA = 1e-3              # 收敛阈值
EPISODES = 100            # 最大迭代次数
```

### 调优建议

**如果训练时间过长**：
- 增大 `THETA` 到 `1e-2`（降低精度要求）
- 减少 `EPISODES` 到 `50`
- 减少 `GAMMA` 到 `0.85`（更重视短期奖励）

**如果内存不足**：
- 修改策略表使用 `float32` 代替 `float64`

## 检查清单

训练前检查：

- [ ] 测试脚本运行成功（`test_state_encoding.py`）
- [ ] 配置文件正确（`train_env_conf.toml`）
- [ ] 算法参数合理（`conf.py`）
- [ ] 有足够内存（至少2GB可用）
- [ ] 有足够时间（可能需要30-60分钟）

训练后检查：

- [ ] 值迭代收敛（delta < theta）
- [ ] 模型文件保存成功
- [ ] 日志无错误信息
- [ ] 评估分数 > 1000

## 常见问题

### Q1: 为什么训练时间这么长？

**A**: 420万状态空间很大，每次迭代需要遍历所有状态。这是正常的，预计30-60分钟。

### Q2: 能否加速训练？

**A**: 可以尝试：
1. 增大收敛阈值（theta）
2. 减少最大迭代次数
3. 使用更快的硬件

### Q3: 内存占用多少？

**A**: 
- 值函数：~34 MB
- 策略表：~134 MB
- 总计：~168 MB（可接受）

### Q4: 第二次测评如何处理？

**A**: 第二次测评是8个随机宝箱。当前实现考虑了所有10个宝箱的组合，因此：
- 策略已经包含了所有可能的宝箱组合
- 无需重新训练
- 直接使用相同模型即可

### Q5: 如何验证策略正确性？

**A**: 
1. 查看训练日志，确认收敛
2. 运行评估，查看是否收集宝箱
3. 检查分数是否 > 1000

## 技术支持

详细技术文档：
- `实验详细分析.md` - 完整的实验分析
- `动态规划实现说明.md` - 实现细节说明

关键代码位置：
- 状态编码：`agent.py` 第48-71行
- 状态转移：`algorithm.py` 第253-315行
- 值迭代：`algorithm.py` 第95-176行

## 下一步

完成训练后：
1. 提交模型到腾讯开悟平台
2. 等待第一次测评（11月15日）
3. 根据结果分析，必要时微调参数
4. 准备第二次测评（11月17日）

---

**祝训练顺利！预期效果：1500+分！** 🎯

