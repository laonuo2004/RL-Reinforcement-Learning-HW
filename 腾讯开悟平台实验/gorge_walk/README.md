# gorge_walk

详细实验说明见[峡谷漫步开发指南](..\docs\峡谷漫步开发指南.md)。

地图如下：

![地图](images/map.png)

4种算法选一个，或者自己 DIY。我选用动态规划，不得不说训练起来很快，迭代优化很方便，结果也很好。尤其是测评二随机环境，其他方法都不好 train，最终还是靠动态规划取巧拿到了很不错的分数。

## 关于测评

两次测评，第一次是固定环境，第二次是随机环境，参数设置：

- 第一次：最大步长 1999，10个宝箱全部生成，理论最高分为 1491.8
- 第二次：最大步长 1800，生成8个随机宝箱，理论最高分大约在 1250~1260 左右

第二次测评比第一次测评晚一两天，中途可以换模型。

最终分数取平均值。

## 关于思路

关于测评一，最开始我还是按照正常动态规划的写法来写的，即列出所有状态，设定好奖励，然后迭代优化。但是总状态数高达 $64 \times 64 \times 1024 = 4194304$，训练差不多要一个多小时，算不完；后来优化了状态空间，仅考虑可达的状态，大概一百多万个，训练时间15分钟左右，效果不错，有1470多分，不过不是最优解。后来发现完全可以采取一些取巧的手段：

首先，可以将问题建模成类似 TSP 的图论问题，即有起点+终点+宝箱生成点共12个节点，节点间均有无向边相连。我们希望找到一条从起点到终点，经过所有宝箱生成点，且总距离最短的路径。求解这个问题可以得到理论最优路径 (依次收集宝箱: `[0, 1, 2, 4, 5, 9, 8, 3, 6, 7]`，总步数 290，分数 1491.8)，然后我们强行让 agent 学会走这条路径就可以了。

于是可以将状态划分为 11 个阶段，依次是收集了 0 个宝箱、1 个宝箱、...、10 个宝箱，每个阶段都有各自的终点，即最优路径上对应的下一个宝箱生成点。这样一来，总状态数降为 $11 \times 64 \times 64 = 45056$，可以接受。最后训练时间大概 10 秒钟就训完了，非常快，评估也是顺利拿到 1491 分。

![1](images/1.png)

---

关于测评二，由于是随机环境，所以一般方法都不好做，很容易终点都走不到。但是我们可以把测评一的模型**原封不动**地搬过来直接用，并且效果也非常优异 (均分 1250+，和理论最高分十分接近)。因为我们的智能体本质上就是学会走最优路径，而这条路径是固定的，所以随机环境不会产生什么影响。并且由于计分规则里走一步 -0.2 分，加上地图小，所以虽然可能会绕点路，但是扣不了多少分，分数总体还是非常高。

![2](images/2.png)