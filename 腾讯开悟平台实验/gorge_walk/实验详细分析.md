# 峡谷漫步实验详细分析文档

## 一、实验整体架构

### 1.1 实验目标
峡谷漫步是一个强化学习任务，目标是训练智能体：
- **从起点走到终点**（必须完成）
- **收集宝箱**（可选，但能提高分数）
- **在最大步数内完成**（否则得分为0）

### 1.2 评分机制
```
总积分 = 终点积分 + 步数积分 + 宝箱积分

- 终点积分：到达终点即获得 150 积分
- 步数积分：(最大步数 - 完成步数) × 0.2
- 宝箱积分：每获得一个宝箱，增加 100 积分
```

**关键约束**：若在最大步数内没有走到终点，则总积分为 0。

### 1.3 代码包结构
```
gorge_walk/
├── agent_dynamic_programming/    # 动态规划算法
│   ├── agent.py                  # 智能体实现
│   ├── algorithm/                # 算法实现
│   │   └── algorithm.py
│   ├── feature/                  # 特征处理
│   │   └── definition.py
│   ├── workflow/                 # 训练工作流
│   │   └── train_workflow.py
│   └── conf/                     # 配置文件
├── agent_q_learning/             # Q-Learning算法（已实现宝箱收集）
├── agent_sarsa/                  # SARSA算法
├── agent_monte_carlo/            # 蒙特卡洛算法
├── conf/
│   └── map_data/                 # 地图数据
│       ├── F_level_0.json        # 简单地图
│       └── F_level_1.json        # 完整地图
└── train_test.py                 # 代码测试脚本
```

---

## 二、地图数据格式详解

### 2.1 地图基本信息
- **地图大小**：64 × 64 网格
- **状态编码**：`state = x * 64 + z`（x是行，z是列）
- **动作空间**：4个动作
  - `0`: UP（上）
  - `1`: DOWN（下）
  - `2`: LEFT（左）
  - `3`: RIGHT（右）

### 2.2 地图数据文件格式
地图数据存储在 `conf/map_data/F_level_1.json` 中，格式如下：

```json
{
  "475": {
    "0": [476, 0, false],
    "1": [475, 0, false],
    "2": [475, 0, false],
    "3": [539, 0, false]
  }
}
```

**格式说明**：
- 外层键：状态ID（字符串格式）
- 内层键：动作ID（"0", "1", "2", "3"）
- 值：`[下一个状态ID, 奖励, 是否到达终点]`
  - 下一个状态ID：执行该动作后到达的状态
  - 奖励：通常为0，到达终点时为150
  - 是否到达终点：布尔值，true表示到达终点

**特殊值说明**：
- 如果某个状态-动作对不存在，表示该动作不可执行（遇到障碍物）
- 奖励为0通常表示普通移动，但代码中会将其转换为-1（步数惩罚）
- 到达终点时，奖励为150，第三个值为true

### 2.3 宝箱位置信息
根据文档，10个宝箱的固定坐标如下：

| 宝箱ID | 坐标 [x, z] | 状态ID (x*64+z) |
|--------|------------|----------------|
| 0 | [19, 14] | 1230 |
| 1 | [9, 28] | 604 |
| 2 | [9, 44] | 620 |
| 3 | [42, 45] | 2733 |
| 4 | [32, 23] | 2071 |
| 5 | [49, 56] | 3192 |
| 6 | [35, 58] | 2298 |
| 7 | [23, 55] | 1527 |
| 8 | [41, 33] | 2657 |
| 9 | [54, 41] | 3497 |

**注意**：宝箱的收集不是通过地图数据直接表示的，而是通过环境的状态信息（`treasure_status`）来跟踪。

---

## 三、环境接口详解

### 3.1 环境初始化
```python
observation, state = env.reset(usr_conf=usr_conf)
```

**usr_conf 配置项**：
- `start`: 起点坐标 [x, z]
- `end`: 终点坐标 [x, z]
- `treasure_random`: 是否随机生成宝箱（bool）
- `treasure_count`: 随机宝箱数量（0~10）
- `treasure_id`: 固定宝箱ID列表（[0, 1, 2, ...]）
- `max_step`: 最大步数（1~2000）

### 3.2 环境执行动作
```python
frame_no, observation, score, terminated, truncated, state = env.step(act)
```

**返回值说明**：
- `frame_no`: 当前帧数
- `observation`: 局部观测（包含特征数据）
- `score`: 得分信息（用于评估，不是强化学习的reward）
- `terminated`: 是否正常结束（到达终点）
- `truncated`: 是否异常中断（超时等）
- `state`: 全局状态信息

### 3.3 观测数据（observation）
`observation` 包含：
- `feature`: 特征向量（250维）
- `legal_act`: 合法动作列表

### 3.4 全局状态（state）
`state` 包含：
- `frame_state`: 环境帧数据
- `game_info`: 游戏信息
  - `pos_x`, `pos_z`: 当前位置坐标
  - `local_view`: 局部视野信息（5×5）
  - `location_memory`: 位置记忆信息
  - `treasure_status`: 宝箱状态（长度为10的列表，1表示可收集，0表示不可收集）

---

## 四、特征提取详解

### 4.1 特征向量组成（250维）
根据文档，特征向量包含以下部分：

| 特征名称 | 维度 | 说明 |
|---------|------|------|
| state | [0:1] | 当前位置状态ID（1维） |
| pos_row | [1:65] | 行坐标的one-hot编码（64维） |
| pos_col | [65:129] | 列坐标的one-hot编码（64维） |
| end_dist | [129:130] | 到终点的离散化距离（1维，0-6） |
| treasure_dist | [130:140] | 到各宝箱的离散化距离（10维，0-6） |
| obstacle_flat | [140:165] | 局部视野障碍物信息（25维，5×5展平） |
| treasure_flat | [165:190] | 局部视野宝箱信息（25维） |
| end_flat | [190:215] | 局部视野终点信息（25维） |
| memory_flat | [215:240] | 局部视野记忆信息（25维） |
| treasure_status | [240:250] | 宝箱状态（10维） |

### 4.2 动态规划算法的特征处理
**当前实现**（`agent_dynamic_programming/agent.py`）：
```python
def observation_process(self, raw_obs, game_info):
    pos = [game_info.pos_x, game_info.pos_z]
    state = [int(pos[0] * 64 + pos[1])]  # 只使用位置信息
    # ... 其他特征被忽略
    return ObsData(feature=int(raw_obs[0]))  # 只返回状态ID
```

**问题**：动态规划算法**只使用了位置信息**，完全忽略了宝箱状态，因此无法实现宝箱收集。

### 4.3 Q-Learning算法的特征处理
**Q-Learning实现**（`agent_q_learning/agent.py`）：
```python
def observation_process(self, raw_obs, game_info):
    # ... 提取所有特征
    pos = int(raw_obs[0])
    treasure_status = [int(item) for item in raw_obs[-10:]]
    # 关键：将位置和宝箱状态组合成新状态
    state = 1024 * pos + sum([treasure_status[i] * (2**i) for i in range(10)])
    return ObsData(feature=int(state))
```

**关键设计**：
- 状态 = `1024 * 位置ID + 宝箱状态编码`
- 宝箱状态编码：将10个宝箱的状态（0或1）编码为一个二进制数
- 这样可以将"位置+已收集宝箱"的组合作为状态

**状态空间大小**：
- 位置数：64 × 64 = 4096
- 宝箱状态组合：2^10 = 1024
- 总状态数：4096 × 1024 = 4,194,304（约420万）

---

## 五、动态规划算法详解

### 5.1 算法实现位置
- **算法类**：`agent_dynamic_programming/algorithm/algorithm.py` 中的 `Algorithm` 类
- **训练入口**：`agent_dynamic_programming/workflow/train_workflow.py` 中的 `workflow` 函数

### 5.2 算法流程

#### 5.2.1 训练工作流
```python
@attached
def workflow(envs, agents, logger=None, monitor=None):
    # 1. 读取地图数据
    map_data = read_map_data("conf/map_data/F_level_1.json")
    
    # 2. 直接调用 learn，传入地图数据（状态转移函数）
    agent.learn(map_data)
    
    # 3. 保存模型
    agent.save_model()
```

**关键点**：
- 动态规划算法**不需要与环境交互**，直接使用地图数据（状态转移函数F）进行计算
- 训练是一次性的，计算出最优策略后保存

#### 5.2.2 算法核心方法

**值迭代（Value Iteration）**：
```python
def value_iteration(self, F):
    V = np.zeros(self.state_size)  # 初始化值函数
    
    for i in range(self.episodes):
        delta = 0
        for state in range(self.state_size):
            v = V[state]
            # 贝尔曼最优方程
            V[state] = max(self._get_value(state, action, F, V) 
                          for action in range(self.action_size))
            delta = max(delta, abs(v - V[state]))
        
        if delta < self.theta:  # 收敛判断
            break
        
        # 计算最优策略
        Q = self.q_value_iteration(V, F)
        policy = self.policy_improvement(Q)
    
    self.agent_policy = policy
    return policy, V
```

**策略迭代（Policy Iteration）**：
```python
def policy_iteration(self, F):
    policy = np.ones([self.state_size, self.action_size]) / self.action_size
    
    for i in range(self.episodes):
        # 1. 策略评估：计算当前策略的值函数
        V = self.policy_evaluation(policy, F)
        
        # 2. 计算Q值
        Q = self.q_value_iteration(V, F)
        
        # 3. 策略改进：贪心策略
        new_policy = self.policy_improvement(Q)
        
        if np.allclose(policy, new_policy):  # 策略收敛
            break
        
        policy = new_policy
    
    self.agent_policy = policy
    return policy, V
```

**状态-动作值计算**：
```python
def _get_value(self, state, action, F, V):
    try:
        next_state, reward, _ = F[str(state)][str(action)]
        if reward == 0:
            reward = -1  # 普通移动惩罚
        value = reward + self.gamma * V[next_state]
    except KeyError:
        pass  # 动作不可执行
    return value
```

### 5.3 当前实现的问题

#### 问题1：状态空间设计不足
- **当前**：只使用位置信息（4096个状态）
- **问题**：无法区分"已收集哪些宝箱"的状态
- **结果**：算法只能找到从起点到终点的最短路径，无法考虑宝箱

#### 问题2：奖励设计不足
- **当前**：普通移动奖励为-1，到达终点奖励为150
- **问题**：没有宝箱收集的奖励
- **结果**：算法不会主动收集宝箱

#### 问题3：状态转移函数不完整
- **当前**：只使用地图数据F，不包含宝箱收集后的状态变化
- **问题**：宝箱收集会改变`treasure_status`，从而改变状态，但当前实现没有考虑这一点

---

## 六、Q-Learning算法对比分析

### 6.1 Q-Learning的状态设计
```python
# 状态 = 位置编码 + 宝箱状态编码
state = 1024 * pos + sum([treasure_status[i] * (2**i) for i in range(10)])
```

**优势**：
- 能够区分不同的宝箱收集状态
- 状态空间虽然大（420万），但可以通过Q表学习

### 6.2 Q-Learning的奖励设计
Q-Learning在`reward_shaping`函数中设计了奖励，虽然代码中没有直接展示，但从行为可以推断：
- 收集宝箱有正奖励
- 到达终点有正奖励
- 普通移动可能有小惩罚

### 6.3 Q-Learning的学习过程
```python
def learn(self, list_sample_data):
    sample = list_sample_data[0]
    state, action, reward, next_state = ...
    
    # Q-Learning更新公式
    delta = reward + self.gamma * np.max(self.Q[next_state, :]) - self.Q[state, action]
    self.Q[state, action] += self.learning_rate * delta
```

**特点**：
- 通过与环境交互收集样本
- 使用TD学习更新Q值
- 需要大量样本才能收敛

---

## 七、动态规划算法优化方案

### 7.1 状态空间扩展

**方案1：完整状态空间（推荐用于第一次测评）**
```python
# 状态编码：位置 + 宝箱状态
state = pos * (2 ** 10) + treasure_state_encoding

# 其中：
# - pos: 位置ID (0-4095)
# - treasure_state_encoding: 10位二进制数，每位表示一个宝箱是否已收集
# - 总状态数：4096 * 1024 = 4,194,304
```

**状态空间大小**：
- 第一次测评：10个宝箱全部生成，状态数 = 4096 × 1024 = 4,194,304
- 第二次测评：8个随机宝箱，但状态数仍然是 4,194,304（因为需要处理所有可能的宝箱组合）

### 7.2 状态转移函数扩展

需要修改地图数据，使其包含宝箱收集的信息：

```python
# 伪代码
def get_transition(state, action, map_data, treasure_positions):
    pos = state // (2 ** 10)
    treasure_status = state % (2 ** 10)
    
    # 获取下一个位置
    next_pos, reward, done = map_data[pos][action]
    
    # 检查是否收集到宝箱
    next_treasure_status = treasure_status
    treasure_reward = 0
    if next_pos in treasure_positions:
        treasure_id = treasure_positions.index(next_pos)
        if (treasure_status >> treasure_id) & 1 == 1:  # 宝箱未收集
            next_treasure_status = treasure_status & ~(1 << treasure_id)  # 标记为已收集
            treasure_reward = 100  # 宝箱奖励
    
    # 组合新状态
    next_state = next_pos * (2 ** 10) + next_treasure_status
    total_reward = reward + treasure_reward
    
    return next_state, total_reward, done
```

### 7.3 奖励设计

```python
def _get_value(self, state, action, F, V, treasure_positions):
    pos = state // (2 ** 10)
    treasure_status = state % (2 ** 10)
    
    try:
        next_pos, base_reward, done = F[str(pos)][str(action)]
        
        # 基础奖励
        if base_reward == 0:
            reward = -1  # 步数惩罚
        else:
            reward = base_reward  # 到达终点奖励150
        
        # 宝箱奖励
        if next_pos in treasure_positions:
            treasure_id = treasure_positions.index(next_pos)
            if (treasure_status >> treasure_id) & 1 == 1:
                reward += 100  # 收集宝箱奖励
        
        # 计算下一个状态
        next_treasure_status = treasure_status
        if next_pos in treasure_positions:
            treasure_id = treasure_positions.index(next_pos)
            if (treasure_status >> treasure_id) & 1 == 1:
                next_treasure_status = treasure_status & ~(1 << treasure_id)
        
        next_state = next_pos * (2 ** 10) + next_treasure_status
        value = reward + self.gamma * V[next_state]
        
    except KeyError:
        value = -float('inf')  # 不可执行的动作
    
    return value
```

### 7.4 特征处理修改

```python
def observation_process(self, raw_obs, game_info):
    pos = [game_info.pos_x, game_info.pos_z]
    state_id = int(pos[0] * 64 + pos[1])
    
    # 获取宝箱状态
    treasure_status = game_info.treasure_status  # [0,1,0,1,...] 长度为10
    
    # 编码宝箱状态为二进制数
    treasure_encoding = sum([treasure_status[i] * (2 ** i) for i in range(10)])
    
    # 组合状态
    full_state = state_id * (2 ** 10) + treasure_encoding
    
    return ObsData(feature=int(full_state))
```

### 7.5 预测和利用方法

```python
def predict(self, state):
    # 直接使用最优策略
    return np.argmax(self.algorithm.agent_policy[state])

def exploit(self, state):
    # 评估时也使用最优策略
    return np.argmax(self.algorithm.agent_policy[state])
```

### 7.6 内存和计算优化

**问题**：420万状态 × 4动作 = 1680万个Q值，内存占用较大

**优化方案**：
1. **稀疏存储**：只存储可达状态
2. **迭代优化**：使用更高效的迭代方法
3. **并行计算**：利用多核CPU加速
4. **收敛加速**：使用更激进的收敛条件

**代码优化示例**：
```python
# 使用字典存储可达状态
self.V = {}  # 只存储计算过的状态
self.agent_policy = {}  # 只存储可达状态的策略

# 在值迭代中，只遍历可达状态
reachable_states = self._get_reachable_states(map_data, start_state)
for state in reachable_states:
    # ... 更新值函数
```

---

## 八、实现步骤建议

### 8.1 第一步：理解现有代码
1. 仔细阅读 `agent_dynamic_programming` 目录下的所有代码
2. 理解 `agent_q_learning` 中宝箱收集的实现
3. 理解地图数据格式和状态转移

### 8.2 第二步：修改特征处理
1. 修改 `observation_process`，加入宝箱状态编码
2. 确保状态空间正确扩展

### 8.3 第三步：扩展状态转移函数
1. 创建新的状态转移函数，考虑宝箱收集
2. 修改 `_get_value` 方法，加入宝箱奖励

### 8.4 第四步：优化算法实现
1. 考虑使用稀疏存储
2. 优化迭代过程
3. 添加进度监控

### 8.5 第五步：测试和验证
1. 使用 `train_test.py` 测试代码正确性
2. 在本地环境测试训练过程
3. 评估模型性能

### 8.6 第六步：针对测评优化
1. **第一次测评**（10个固定宝箱）：
   - 使用完整状态空间
   - 预先计算所有可能状态的最优策略
   
2. **第二次测评**（8个随机宝箱）：
   - 可能需要动态适应
   - 或者为每种可能的宝箱组合预计算策略

---

## 九、关键代码修改点总结

### 9.1 必须修改的文件

1. **`agent_dynamic_programming/agent.py`**
   - `observation_process`: 加入宝箱状态编码
   - `predict`/`exploit`: 确保使用扩展后的状态

2. **`agent_dynamic_programming/algorithm/algorithm.py`**
   - `_get_value`: 加入宝箱奖励和状态转移
   - `value_iteration`/`policy_iteration`: 处理扩展后的状态空间

3. **`agent_dynamic_programming/conf/conf.py`**
   - `STATE_SIZE`: 修改为 4096 * 1024 = 4,194,304

### 9.2 可能需要修改的文件

1. **`agent_dynamic_programming/workflow/train_workflow.py`**
   - 可能需要传入宝箱位置信息
   - 可能需要优化训练过程

2. **`agent_dynamic_programming/feature/definition.py`**
   - 可能需要修改数据结构

---

## 十、注意事项

### 10.1 状态空间爆炸
- 420万状态可能导致内存不足
- 建议使用稀疏存储或只计算可达状态

### 10.2 计算时间
- 值迭代可能需要较长时间
- 建议添加进度监控和日志

### 10.3 宝箱位置获取
- 需要从环境配置或地图数据中获取宝箱位置
- 第一次测评：10个固定宝箱位置已知
- 第二次测评：8个随机宝箱，需要动态获取

### 10.4 模型保存
- 确保模型文件大小合理
- 考虑使用压缩或只保存策略表

---

## 十一、理论分析

### 11.1 为什么动态规划能保证最优？
- 动态规划基于**贝尔曼最优方程**，在已知完整环境模型的情况下，可以保证找到**全局最优策略**
- 值迭代和策略迭代都能收敛到最优解

### 11.2 与Q-Learning的对比
- **动态规划**：需要完整环境模型，计算复杂度高，但保证最优
- **Q-Learning**：不需要环境模型，通过交互学习，但可能不是最优解

### 11.3 为什么当前DP实现分数低？
- **只考虑了位置**，没有考虑宝箱状态
- **奖励设计不完整**，没有宝箱奖励
- **状态转移不完整**，没有处理宝箱收集

---

## 十二、总结

动态规划算法理论上可以找到最优策略，但当前实现存在以下问题：

1. **状态空间设计不足**：只使用位置，忽略了宝箱状态
2. **奖励设计不完整**：没有宝箱收集奖励
3. **状态转移不完整**：没有处理宝箱收集后的状态变化

**解决方案**：
1. 扩展状态空间，包含宝箱状态
2. 修改状态转移函数，考虑宝箱收集
3. 优化算法实现，处理大规模状态空间

**预期效果**：
- 第一次测评：能够收集所有10个宝箱，找到最优路径
- 分数应该能够超过Q-Learning的1027分

---

## 附录：关键代码片段参考

### A.1 状态编码（参考Q-Learning）
```python
pos = int(raw_obs[0])
treasure_status = [int(item) for item in raw_obs[-10:]]
state = 1024 * pos + sum([treasure_status[i] * (2**i) for i in range(10)])
```

### A.2 宝箱位置映射
```python
TREASURE_POSITIONS = {
    0: 1230,  # [19, 14]
    1: 604,   # [9, 28]
    2: 620,   # [9, 44]
    3: 2733,  # [42, 45]
    4: 2071,  # [32, 23]
    5: 3192,  # [49, 56]
    6: 2298,  # [35, 58]
    7: 1527,  # [23, 55]
    8: 2657,  # [41, 33]
    9: 3497   # [54, 41]
}
```

### A.3 状态解码
```python
def decode_state(state):
    pos = state // (2 ** 10)
    treasure_encoding = state % (2 ** 10)
    treasure_status = [(treasure_encoding >> i) & 1 for i in range(10)]
    return pos, treasure_status
```

---

**文档结束**

