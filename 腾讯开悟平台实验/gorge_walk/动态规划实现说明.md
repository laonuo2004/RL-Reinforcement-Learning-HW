# 动态规划算法实现说明

## 概述

本文档说明如何实现一个考虑所有10个宝箱的动态规划算法，用于峡谷漫步任务。

## 核心修改

### 1. 状态空间扩展

**原始实现**：
- 状态数：4,096（仅考虑位置）
- 状态编码：`state = x * 64 + z`

**新实现**：
- 状态数：4,194,304（位置 × 宝箱组合）
- 状态编码：`state = pos_id * 1024 + treasure_encoding`
  - `pos_id`：位置ID (0-4095)
  - `treasure_encoding`：宝箱状态的二进制编码 (0-1023)
    - 第i位为1表示宝箱i可收集
    - 第i位为0表示宝箱i已收集或未生成

**示例**：
```python
# 位置：[29, 9]（起点），所有宝箱可收集
pos_id = 29 * 64 + 9 = 1865
treasure_status = [1,1,1,1,1,1,1,1,1,1]
treasure_encoding = 2^0 + 2^1 + ... + 2^9 = 1023
state = 1865 * 1024 + 1023 = 1,910,783

# 位置：[19, 14]（宝箱0），已收集宝箱0
pos_id = 19 * 64 + 14 = 1230
treasure_status = [0,1,1,1,1,1,1,1,1,1]
treasure_encoding = 2^1 + 2^2 + ... + 2^9 = 1022
state = 1230 * 1024 + 1022 = 1,259,518
```

### 2. 奖励设计

**新奖励机制**（与评分机制一致）：
- 普通移动：**-0.2**（而不是原来的-1）
- 收集宝箱：**+100**
- 到达终点：**+150**

**理由**：
- 评分机制中，每步损失0.2分：`步数积分 = (最大步数 - 完成步数) × 0.2`
- 宝箱价值100分，相当于500步的价值
- 地图小，宝箱密集，收集宝箱收益高

### 3. 状态转移函数

**核心逻辑**（`_get_value` 方法）：

```python
def _get_value(self, state, action, F, V):
    # 1. 解码状态
    pos_id = state // 1024
    treasure_encoding = state % 1024
    
    # 2. 获取基础状态转移（从地图数据）
    next_pos_id, base_reward, done = F[str(pos_id)][str(action)]
    
    # 3. 计算移动奖励
    reward = -0.2 if base_reward == 0 else base_reward
    
    # 4. 检查宝箱收集
    next_treasure_encoding = treasure_encoding
    if next_pos_id in treasure_positions:
        treasure_id = treasure_positions.index(next_pos_id)
        if (treasure_encoding >> treasure_id) & 1 == 1:
            reward += 100  # 收集奖励
            next_treasure_encoding &= ~(1 << treasure_id)  # 更新状态
    
    # 5. 编码新状态
    next_state = next_pos_id * 1024 + next_treasure_encoding
    
    # 6. 贝尔曼方程
    value = reward + gamma * V[next_state]
    
    return value
```

## 修改的文件

### 1. `agent_dynamic_programming/conf/conf.py`

**修改内容**：
- 添加 `POSITION_SIZE = 4096`
- 添加 `TREASURE_COMBINATIONS = 1024`
- 修改 `STATE_SIZE = 4,194,304`
- 添加 `TREASURE_POSITIONS` 列表（10个宝箱的位置ID）

### 2. `agent_dynamic_programming/agent.py`

**修改内容**：
- `observation_process` 方法：加入宝箱状态编码
  ```python
  pos_id = int(pos[0] * 64 + pos[1])
  treasure_status = game_info.treasure_status
  treasure_encoding = sum([treasure_status[i] * (2**i) for i in range(10)])
  full_state = pos_id * Config.TREASURE_COMBINATIONS + treasure_encoding
  return ObsData(feature=int(full_state))
  ```

### 3. `agent_dynamic_programming/algorithm/algorithm.py`

**修改内容**：
- `__init__` 方法：加入宝箱位置信息
- `_get_value` 方法：完整的状态转移和奖励计算
- `value_iteration` 方法：添加详细日志输出

## 内存和性能

### 内存占用

- 值函数表 V：`4,194,304 × 8 bytes = 33.6 MB`（float64）
- 策略表 policy：`4,194,304 × 4 × 8 bytes = 134.2 MB`（float64）
- **总计：约 168 MB**

使用 float32 可以减半到约 84 MB。

### 计算时间

- 每次迭代需要遍历 4,194,304 个状态
- 每个状态需要计算 4 个动作的值
- 预计每次迭代时间：数分钟（取决于硬件）
- 收敛迭代次数：通常在10-100次之间

**优化建议**（暂未实现）：
- 使用稀疏存储（只存储可达状态）
- 并行计算（利用多核CPU）
- 降低收敛阈值

## 使用方法

### 1. 测试状态编码

运行测试脚本：
```bash
python test_state_encoding.py
```

这会验证：
- 状态编码/解码的正确性
- 状态空间大小
- 内存估计
- 宝箱位置信息

### 2. 训练模型

修改 `train_test.py` 中的算法名称：
```python
algorithm_name = "dynamic_programming"
```

运行训练：
```bash
python train_test.py
```

**注意**：
- 训练可能需要较长时间（可能数十分钟到一小时）
- 监控日志输出，查看迭代进度
- 确保有足够的内存（至少2GB可用）

### 3. 配置环境参数

在 `agent_dynamic_programming/conf/train_env_conf.toml` 中配置：

**第一次测评配置**：
```toml
start = [29, 9]
end = [11, 55]
treasure_id = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
treasure_random = false
max_step = 1999
```

**第二次测评配置**：
```toml
start = [29, 9]
end = [11, 55]
treasure_count = 8
treasure_random = true
max_step = 1800
```

## 理论分析

### 为什么能保证最优？

1. **完整环境模型**：地图数据提供了完整的状态转移函数
2. **贝尔曼最优方程**：值迭代基于贝尔曼方程，保证收敛到最优值函数
3. **确定性环境**：环境是完全确定的，没有随机性

### 预期效果

**第一次测评**（10个固定宝箱）：
- 理论上能收集所有10个宝箱
- 找到从起点到终点经过所有宝箱的最优路径
- 最大化总积分：`150 + (1999-步数)×0.2 + 10×100`

**保守估计**：
- 假设收集10个宝箱需要约200步
- 总积分 ≈ 150 + (1999-200)×0.2 + 1000 = 150 + 360 + 1000 = **1510分**

**这将明显超过Q-Learning的1027分！**

### 与Q-Learning对比

| 特性 | 动态规划 | Q-Learning |
|------|---------|------------|
| 需要环境模型 | 是 | 否 |
| 保证最优 | 是 | 否 |
| 训练时间 | 长（一次性） | 中等 |
| 内存占用 | 大 | 大 |
| 适用场景 | 已知环境 | 未知环境 |

## 调试和监控

### 日志输出

训练过程中会输出详细日志：
```
Starting value iteration with state_size=4194304
Iteration 0/100, delta=150.000000, iter_time=45.23s, elapsed=45.23s
Iteration 10/100, delta=23.456789, iter_time=43.87s, elapsed=452.30s
...
Converged at iteration 45 with delta=0.000999
Value iteration completed in 2034.56s (45 iterations)
```

### 关键指标

- **delta**：每次迭代的值函数变化量，越小表示越接近收敛
- **iter_time**：每次迭代的时间
- **elapsed**：总耗时
- **est_remaining**：预计剩余时间

### 常见问题

**1. 内存不足**
- 检查系统可用内存（需要至少2GB）
- 考虑使用float32代替float64

**2. 训练时间过长**
- 正常现象，420万状态需要时间
- 可以降低收敛阈值（theta）加速收敛
- 可以减少最大迭代次数

**3. 收敛慢**
- 检查gamma值（当前0.9）
- 检查theta值（当前1e-3）
- 可以适当放宽收敛条件

## 后续优化方向

如果当前实现效果良好，可以考虑以下优化：

1. **稀疏存储**：只存储可达状态（大幅减少内存和计算）
2. **并行计算**：利用多核CPU加速值迭代
3. **策略迭代**：可能比值迭代更快收敛
4. **异步更新**：不需要同步更新所有状态
5. **优先级队列**：优先更新值变化大的状态

## 总结

本实现：
- ✅ 扩展状态空间，考虑宝箱收集
- ✅ 修改奖励机制，与评分一致
- ✅ 实现完整的状态转移函数
- ✅ 添加详细的日志监控
- ✅ 理论上保证最优策略

**预期能够显著超越Q-Learning的1027分，达到1500+分！**

---

**文档完成日期**：2025-11-13

